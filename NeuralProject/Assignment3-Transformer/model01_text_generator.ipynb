{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "AXY82Qm0LQ4Z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763644888917,
     "user_tz": -120,
     "elapsed": 12647,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "67b6047d-14c4-45ec-806f-6262f5d7263c",
    "ExecuteTime": {
     "end_time": "2025-11-20T19:59:20.460464Z",
     "start_time": "2025-11-20T19:59:16.685467Z"
    }
   },
   "source": [
    "\n",
    "# Cell 1: Setup and imports\n",
    "import numpy as np\n",
    "import keras\n",
    "import sentencepiece as spm\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 21:59:17.276081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-20 21:59:17.449849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763668757.517829     623 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763668757.536445     623 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763668757.679353     623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763668757.679376     623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763668757.679377     623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763668757.679377     623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-20 21:59:17.695572: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "data = \"/content/drive/My Drive/NeuroProjekt/Transformerit/Data/raamattu_clean.txt\"'''"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0S7T8Azha5v_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763644916239,
     "user_tz": -120,
     "elapsed": 23689,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "141221cf-91b9-4d00-81cb-0a9c09a3d0d7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Cell 2: Load and prepare text data\n",
    "# Load the Raamattu text\n",
    "data = './raamattu_clean.txt'\n",
    "with open(data, 'r', encoding='utf-8-sig') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"First 100 characters: {text[:100]}\")"
   ],
   "metadata": {
    "id": "7hI6ti3nQLzr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763644929526,
     "user_tz": -120,
     "elapsed": 2285,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "e495ee29-c4d8-4c86-e405-fbe465e027ac",
    "ExecuteTime": {
     "end_time": "2025-11-20T19:59:21.095044Z",
     "start_time": "2025-11-20T19:59:21.055926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 3972786 characters\n",
      "First 100 characters: Alussa Jumala loi taivaan ja maan.\n",
      "\n",
      "Maa oli autio ja tyhjä, pimeys peitti syvyydet, ja Jumalan henki\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "print(\"Load success:\", sp.load(\"/home/ville/tokenizer/raamattu_sp.model\")\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: Train SentencePiece model\n",
    "# Save text to a temporary file for SentencePiece training\n",
    "'''temp_file = 'raamattu_temp.txt'\n",
    "with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Train SentencePiece model\n",
    "vocab_size = 20000  # You can adjust this based on your needs\n",
    "model_prefix = 'raamattu_sp'\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=temp_file,\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    num_threads=32,\n",
    "    character_coverage=1.0,  # Important for Finnish\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['<PAD>', '<UNK>']\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "model_prefix = '~/tokenizer/raamattu_sp'\n",
    "# Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{model_prefix}.model\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Jumala loi taivaan ja maan\"\n",
    "tokens = sp.encode_as_pieces(test_text)\n",
    "print(f\"Tokenized example: {tokens}\")\n",
    "print(f\"Vocabulary size: {sp.get_piece_size()}\")"
   ],
   "metadata": {
    "id": "Wg11Nn0AQNw-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763645092878,
     "user_tz": -120,
     "elapsed": 8904,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "d0434031-f907-4918-d32e-724f09cf2575",
    "ExecuteTime": {
     "end_time": "2025-11-20T19:57:10.346475Z",
     "start_time": "2025-11-20T19:57:10.321060Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"~/tokenizer/raamattu_sp.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# Load the trained tokenizer\u001B[39;00m\n\u001B[32m     25\u001B[39m sp = spm.SentencePieceProcessor()\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[43msp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel_prefix\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.model\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Test tokenization\u001B[39;00m\n\u001B[32m     29\u001B[39m test_text = \u001B[33m\"\u001B[39m\u001B[33mJumala loi taivaan ja maan\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/keras_gpu/lib/python3.11/site-packages/sentencepiece/__init__.py:961\u001B[39m, in \u001B[36mSentencePieceProcessor.Load\u001B[39m\u001B[34m(self, model_file, model_proto)\u001B[39m\n\u001B[32m    959\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m model_proto:\n\u001B[32m    960\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.LoadFromSerializedProto(model_proto)\n\u001B[32m--> \u001B[39m\u001B[32m961\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mLoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/keras_gpu/lib/python3.11/site-packages/sentencepiece/__init__.py:316\u001B[39m, in \u001B[36mSentencePieceProcessor.LoadFromFile\u001B[39m\u001B[34m(self, arg)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mLoadFromFile\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_sentencepiece\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSentencePieceProcessor_LoadFromFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mOSError\u001B[39m: Not found: \"~/tokenizer/raamattu_sp.model\": No such file or directory Error #2"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: Prepare training data\n",
    "# Tokenize the text\n",
    "seq_length = 64\n",
    "pieces = sp.encode_as_ids(text)\n",
    "print(f\"Total tokens: {len(pieces)}\")\n",
    "\n",
    "# Create sequences\n",
    "sequences = []\n",
    "for i in range(0, len(pieces) - seq_length):\n",
    "    # Input: first seq_length tokens, Target: next seq_length tokens (shifted by 1)\n",
    "    sequences.append(pieces[i:i+seq_length+1])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "sequences = np.array(sequences)\n",
    "inputs = sequences[:, :-1]  # All tokens except the last one\n",
    "targets = sequences[:, 1:]  # All tokens except the first one\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "indices = np.arange(len(sequences))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_inputs, train_targets = inputs[train_indices], targets[train_indices]\n",
    "val_inputs, val_targets = inputs[val_indices], targets[val_indices]"
   ],
   "metadata": {
    "id": "PAnz_68_QQmr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763645131133,
     "user_tz": -120,
     "elapsed": 7488,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "996fb709-2ae0-4c04-cfbd-492a4edbae18"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 5: Define the model\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
    "    positions = np.arange(max_len)[:, np.newaxis]\n",
    "    angles = np.arange(d_model)[np.newaxis, :] / d_model\n",
    "    angles = 1 / (10000**angles)\n",
    "\n",
    "    pos_encoding = positions * angles\n",
    "    pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "# Define model parameters\n",
    "embed_dim = 192\n",
    "num_heads = 4\n",
    "ff_dim = 384\n",
    "num_layers = 2\n",
    "\n",
    "# Create the model\n",
    "inputs = keras.Input(shape=(seq_length,))\n",
    "embedding_layer = keras.layers.Embedding(sp.get_piece_size(), embed_dim)(inputs)\n",
    "\n",
    "# Add positional encoding\n",
    "pos_encoding = get_positional_encoding(seq_length, embed_dim)\n",
    "x = embedding_layer + pos_encoding\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(num_layers):\n",
    "    # Multi-head attention with causal mask\n",
    "    attention_output = keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads,\n",
    "        dropout=0.1\n",
    "    )(x, x, use_causal_mask=True)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ffn = keras.Sequential([\n",
    "        keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        keras.layers.Dense(embed_dim),\n",
    "        keras.layers.Dropout(0.1)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "\n",
    "    # Add & Norm\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "\n",
    "# Final output layer\n",
    "outputs = keras.layers.Dense(sp.get_piece_size())(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "uWf-aCgxQUF7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763645183619,
     "user_tz": -120,
     "elapsed": 2830,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "d69b5488-d9a2-482a-b327-09256e36ec10"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 6: Train the model\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, train_targets,\n",
    "    validation_data=(val_inputs, val_targets),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, verbose=1),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1),\n",
    "        keras.callbacks.ModelCheckpoint('raamattu_best_model.keras', save_best_only=True)\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "Ep-MBkG2QeUI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1763645426044,
     "user_tz": -120,
     "elapsed": 228036,
     "user": {
      "displayName": "Teemu Vataja",
      "userId": "15446791908117884729"
     }
    },
    "outputId": "e5253786-d5c9-46a5-8e52-303cb936a7bc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 7: Plot training metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "IKbYwPYiQhec"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Save the model\n",
    "model.save('raamattu_model.keras')\n",
    "print(\"Model saved as 'raamattu_model.keras'\")"
   ],
   "metadata": {
    "id": "pQaMYDFRQjy0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(model, sp, prompt, num_tokens=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text autoregressively.\n",
    "\n",
    "    Args:\n",
    "        temperature: Controls randomness\n",
    "            - Low (0.1-0.5): Focused, repetitive\n",
    "            - Medium (0.7-1.0): Balanced\n",
    "            - High (1.5+): Creative, chaotic\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    input_ids = sp.encode_as_ids(prompt)\n",
    "\n",
    "    # Rest of your generation code stays the same...\n",
    "    if len(input_ids) < seq_length:\n",
    "        padding_length = seq_length - len(input_ids)\n",
    "        input_ids = [0] * padding_length + input_ids\n",
    "    else:\n",
    "        padding_length = 0\n",
    "        input_ids = input_ids[-seq_length:]\n",
    "\n",
    "    # Generated tokens\n",
    "    generated_ids = list(input_ids[padding_length:])\n",
    "\n",
    "    # Generate text token by token\n",
    "    for _ in range(num_tokens):\n",
    "        x = np.array([input_ids])\n",
    "        predictions = model.predict(x, verbose=0)[0]\n",
    "        logits = predictions[-1]\n",
    "        logits = logits / temperature\n",
    "        exp_logits = np.exp(logits - np.max(logits))\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        next_token = np.random.choice(len(probs), p=probs)\n",
    "        generated_ids.append(next_token)\n",
    "        input_ids = input_ids[1:] + [next_token]\n",
    " # Decode the generated sequence\n",
    "    generated_text = sp.decode(generated_ids)\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "id": "ETTWej1pQlom"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 10: Generate sample text\n",
    "prompts = [\n",
    "    \"Jumala loi taivaan ja maan\",\n",
    "    \"Saatana loi\"\n",
    "    # TODO: Ja tähän lisää\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    generated = generate_text(model, sp, prompt, num_tokens=100, temperature=1.2)\n",
    "    print(generated)"
   ],
   "metadata": {
    "id": "tCfDO9K1Qwyq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 11: Load model (if you're starting a new session)\n",
    "# Uncomment these lines to load a previously saved model\n",
    "\"\"\"\n",
    "# Load saved model\n",
    "model = keras.models.load_model('kalevala_model.keras')\n",
    "\n",
    "# Load SentencePiece tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('kalevala_sp.model')\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Mieleni minun tekevi\"\n",
    "generated = generate_text(model, sp, prompt, num_tokens=150, temperature=1.0)\n",
    "print(generated)\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "z5KKJVTXQ0IC"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
