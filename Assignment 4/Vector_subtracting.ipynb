{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment: week 4\n",
    "\n",
    "## Objectives\n",
    "The objectives of this assignment are:\n",
    "\n",
    "1. to learn how to obtain and use pretrained word embeddings\n",
    "2. to gain a better understanding of word vectors\n",
    "Setup"
   ],
   "id": "7e2e3169447bc20a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The GloVe file is loaded and stored in a dictionary where each key is a word and each value is its vector representation. During loading, I noticed that a few lines contained artifacts such as \".\", \"..\", \"...\", or strings like \"1/2\" that are not meaningful words for this task. These lines caused parsing errors because the non-numeric tokens could not be converted into floats. To ensure clean and valid embeddings, I filtered out these irregular entries and kept only the lines where all vector components were valid numeric values.",
   "id": "13896ff8e843a91e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T17:49:32.670496Z",
     "start_time": "2025-11-19T17:48:36.692625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "glove_path = \"./wiki100d.txt\"\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        # skip malformed lines in embedding data\n",
    "        if len(values[1:]) != embedding_dim:\n",
    "            continue\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], dtype=np.float32)\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "        except ValueError as e:\n",
    "            break\n"
   ],
   "id": "3945b143a254c8c8",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:44:10.631040Z",
     "start_time": "2025-11-19T16:44:10.628253Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Loaded vectors:\", len(embeddings_index))",
   "id": "e4fa5f7e40088a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vectors: 1287614\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is functions for calculating cosine similarity of word vectors and for",
   "id": "68d619fe4a21d02a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:44:11.755156Z",
     "start_time": "2025-11-19T16:44:11.752414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def find_most_similar_word(vec, embeddings, exclude=None, top_n = 10, ):\n",
    "\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    similarities = {word: cosine_similarity(vec,emb)\n",
    "                    for word, emb in embeddings.items() if word not in exclude}\n",
    "\n",
    "    return sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]"
   ],
   "id": "bafe11c43c3b8abd",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I noticed that woman vector - man vector + king vector the closest cosine similarity was queen. The model encodes gender and royalty directions in the vector space. “king” − “man” ≈ “queen” − “woman”. Ofcourse I excluded the words that is in vector computing process",
   "id": "7e18c0c969f710fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:55:09.854304Z",
     "start_time": "2025-11-19T16:55:06.389437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "man_vector = embeddings_index[\"man\"]\n",
    "woman_vector = embeddings_index[\"woman\"]\n",
    "king_vector = embeddings_index[\"king\"]\n",
    "\n",
    "\n",
    "true_vector = woman_vector-man_vector+king_vector\n",
    "nearest_word  = find_most_similar_word(true_vector,embeddings_index, exclude={\"king\", \"man\", \"woman\"},top_n=10)"
   ],
   "id": "cde63a4e53ec0a6b",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T16:55:10.338899Z",
     "start_time": "2025-11-19T16:55:10.336701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in nearest_word:\n",
    "    print(word[0], word[1])"
   ],
   "id": "9b41b22a10db9a98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen 0.8120952\n",
      "throne 0.72555846\n",
      "daughter 0.72254604\n",
      "elizabeth 0.70558876\n",
      "wife 0.7013361\n",
      "mother 0.7000919\n",
      "margaret 0.68769395\n",
      "princess 0.6853057\n",
      "monarch 0.67529035\n",
      "niece 0.6662857\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here I calculated winter + sun - snow. As a funny way it can be imagined that when it is winter and sun starts to shine and snow goes away, summer is coming. So taking the indicating thing from winter which is snow is same as taking sun out from the summer.",
   "id": "c7e8119ca29163d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T17:21:23.866210Z",
     "start_time": "2025-11-19T17:21:20.352536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "beer_vector = embeddings_index[\"beer\"]\n",
    "germany_vector = embeddings_index[\"germany\"]\n",
    "leather_vector = embeddings_index[\"leather\"]\n",
    "europe_vector = embeddings_index[\"europe\"]\n",
    "tradition_vector = embeddings_index[\"tradition\"]\n",
    "\n",
    "snow_vector = embeddings_index[\"snow\"]\n",
    "sun_vector = embeddings_index[\"sun\"]\n",
    "winter_vector = embeddings_index[\"winter\"]\n",
    "\n",
    "true_vector = winter_vector+sun_vector-snow_vector\n",
    "nearest_word  = find_most_similar_word(true_vector,embeddings_index, {\"snow\",\"winter\",\"sun\"},10)"
   ],
   "id": "2e132a463046a0e9",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T17:21:24.358564Z",
     "start_time": "2025-11-19T17:21:24.355918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in nearest_word:\n",
    "    print(word[0], word[1])"
   ],
   "id": "5d76811b52f976bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer 0.7168366\n",
      "spring 0.6835645\n",
      "autumn 0.6784056\n",
      "1997 0.6032326\n",
      "1998 0.6010512\n",
      "fall 0.5967329\n",
      "2008 0.5878199\n",
      "beginning 0.58745766\n",
      "2001 0.58440286\n",
      ". 0.5843619\n"
     ]
    }
   ],
   "execution_count": 115
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
